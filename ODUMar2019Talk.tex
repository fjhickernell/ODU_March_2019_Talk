%Talk given at Monte Carlo Methods 2019
\documentclass[10pt,compress,xcolor={usenames,dvipsnames},aspectratio=169]{beamer}
%\documentclass[xcolor={usenames,dvipsnames},aspectratio=169]{beamer} %slides and 
%notes
\usepackage{amsmath,datetime,
	mathtools,
	bbm,
	%mathabx,
	array,
	booktabs,
	xspace,
	calc,
	colortbl,
 	graphicx}
\usepackage[usenames]{xcolor}
\usepackage[giveninits=false,backend=biber,style=nature, maxcitenames =10, mincitenames=9]{biblatex}
\addbibresource{FJHown23.bib}
\addbibresource{FJH23.bib}
\usepackage{newpxtext}
\usepackage[euler-digits,euler-hat-accent]{eulervm}
\usepackage{media9}
\usepackage[autolinebreaks]{mcode}
\usepackage[tikz]{mdframed}


\usetheme{FJHSlimNoFoot169}
\setlength{\parskip}{2ex}
\setlength{\arraycolsep}{0.5ex}

\DeclareMathOperator{\sol}{SOL}
\DeclareMathOperator{\app}{APP}
\DeclareMathOperator{\alg}{ALG}
\DeclareMathOperator{\ACQ}{ACQ}
\DeclareMathOperator{\ERR}{ERR}
\DeclareMathOperator{\COST}{COST}
\DeclareMathOperator{\COMP}{COMP}
\newcommand{\dataN}{\bigl(\hf(\vk_i)\bigr)_{i=1}^n}
\newcommand{\dataNj}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_j}}
\newcommand{\dataNjd}{\bigl(\hf(\vk_i)\bigr)_{i=1}^{n_{j^\dagger}}}
\newcommand{\ERRN}{\ERR\bigl(\dataN,n\bigr)}

\newcommand{\Sapp}{S_{\textup{app}}}
\newcommand{\LambdaStd}{\Lambda^{\textup{std}}}
\newcommand{\LambdaSer}{\Lambda^{\textup{ser}}}
\newcommand{\LambdaAll}{\Lambda^{\textup{all}}}
%\DeclareMathOperator{\spann}{span}
%\DeclareMathOperator{\app}{app}

\providecommand{\HickernellFJ}{H.}


\iffalse
Adaptive Approximation to Multivariate Linear Problems with Inputs Lying in a Cone

Adaptive algorithms are convenient for the practitioner because they automatically determine the computational effort required to satisfy the error criterion.  The function data acquired for constructing the approximate solution are also used to compute a data-based error bound for the approximate solution.  Computation proceeds until this error bound becomes small enough.  If the set of allowed input functions is convex, adaptive algorithms may offer no advantage to non-adaptive algorithms.  We construct an adaptive algorithm for solving a general, linear problem where the input functions lie in a  \emph{non-convex cone}. The stopping criterion is based on theory, not heuristics.  The cone of input functions is defined so that sampling the most important Fourier series coefficients is sufficient to bound the magnitude of the unsampled Fourier coefficients.  We show that our adaptive algorithm is optimal.  We also determine conditions under which the problem is tractable.  This work is related to  the adaptive algorithms developed in \cite{HicEtal14a,HicEtal17a,KunEtal19a}.

\fi

\renewcommand{\OffTitleLength}{-10ex}
\setlength{\FJHThankYouMessageOffset}{-8ex}
\title{The Right Ingredients for \\ Adaptive Function Approximation Algorithms}
\author[]{Fred J. Hickernell}
\institute{Department of Applied Mathematics \\
	Center for Interdisciplinary Scientific Computation \\  Illinois Institute of Technology \\
	\href{mailto:hickernell@iit.edu}{\url{hickernell@iit.edu}} \quad
	\href{http://mypages.iit.edu/~hickernell}{\url{mypages.iit.edu/~hickernell}}}

\thanksnote{with Yuhan Ding, Mac Hyman, Peter Kritzer, and Simon Mak \\
	partially supported by  NSF-DMS-1522687 and NSF-DMS-1638521 (SAMSI)
}
\event{Old Dominion University}
\date[]{March 6, 2020}

\input FJHDef.tex


%Abstract:  When

\newlength{\figwidth}
\setlength{\figwidth}{0.25\textwidth}

\newlength{\figwidthSmall}
\setlength{\figwidthSmall}{0.2\textwidth}

\newcommand{\financePict}{\href{http://i2.cdn.turner.com/money/dam/assets/130611131918-chicago-board-options-exchange-1024x576.jpg}{\includegraphics[width
		= 3cm]{ProgramsImages/130611131918-chicago-board-options-exchange-1024x576.jpg}}}
	
	\newcommand{\scoop}[1]{\parbox{#1}{\includegraphics[width=#1]{IceCreamScoop.eps}}\xspace}
	\newcommand{\smallscoop}{\scoop{1cm}}
	\newcommand{\medscoop}{\scoop{1.8cm}}
	\newcommand{\largescoop}{\scoop{3cm}}
	\newcommand{\ICcone}[1]{\parbox{#1}{\includegraphics[width=#1,angle=270]{MediumWaffleCone.eps}}\xspace}
	\newcommand{\medcone}{\ICcone{1.2cm}}
	\newcommand{\largercone}{\parbox{2.2cm}{\vspace*{-0.2cm}\includegraphics[width=1cm,angle=270]{MediumWaffleCone.eps}}\xspace}
	\newcommand{\largecone}{\ICcone{1.8cm}}
	\newcommand{\smallcone}{\parbox{1.1cm}{\includegraphics[width=0.5cm,angle=270]{MediumWaffleCone.eps}}\xspace}

	

\newcommand{\northeaststuff}[3]{
	\begin{tikzpicture}[remember picture, overlay]
	\node [shift={(-#1 cm,-#2 cm)}]  at (current page.north east){#3};
	\end{tikzpicture}}


\begin{document}
	\tikzstyle{every picture}+=[remember picture]
	\everymath{\displaystyle}

\frame{\titlepage}


\section{Introduction}

\begin{frame}{Problem}

\vspace{-5ex}
\begin{itemize}
    \item Have a \alert{black-box} function routine $f \colon \Omega \subseteq \reals^d \to \reals$, e.g., output of a computer simulation
    
    \item Cost of a function value, $\$(f)$, is \alert{expensive}
    
    \item Want \alert{fixed tolerance algorithm} $\alg: \cc \times (0,\infty) \to \cg$ such that 
    \[
    \norm[\cg]{f - \alg(f,\varepsilon)} \le \varepsilon \qquad \forall f \in \cc \text{ \alert{candidate set}}, \quad \cc \subset \cg
    \]
    where cost of an $\alg(f,\varepsilon)$ value is \alert{cheap}\uncover<2->{, and 
    \begin{gather*}
        \text{\alert{design or node array} }\mX \in \Omega^n \subseteq \reals^{n \times d}, \qquad \text{\alert{function data} }  \vy = f(\mX)\in \reals^n \\
        \vx_{n+1} = \argmax_{\vx \in \Omega} \ACQ(\vx,n,\mX,\vy) \text{ \alert{acquisition function}} \\
        \norm[\cg]{f - \app(n,\mX,\vy)} \le \ERR(n,\mX,\vy) \text{ \alert{data-driven error bound}} \qquad \forall n \in \naturals, \ f \in \cc \\
        n^* = \min \, \{ n \in \naturals \colon \ERR(n,\mX,\vy) \le \varepsilon \} \text{ \alert{stopping criterion}} \\
        \alg(f,\varepsilon) = \app(n^*,\mX,\vy) \text{ \alert{fixed budget approximation}}
    \end{gather*}}
 \end{itemize}
 
 \vspace{-4ex}
 \uncover<2->{\alert{Adaptive} sample size, design, and fixed budget approximation}
    
\end{frame}


\section{Univariate, Low Accuracy}

\begin{frame}<1>[label = Goal, fragile]
	\frametitle{\only<1>{Done Already}\only<2>{Goal}}
	\northeaststuff{3.7}{2.2}{\includegraphics[width=7cm]{ProgramsImages/LinearSpline.eps}}
	
	\vspace{-1ex}
	
	Given 
	
	\vspace{-3ex}
	\begin{itemize}
		\item \alert{function}, $f:[a,b] \to \reals$, 
		\item \alert{error tolerance}, $\varepsilon$, and 
		\item<only@1> \alert{ball} $\cb = \{f \in \cw^{2,\infty}: \norm{f''} \le \alert{\sigma} \}$ of radius \alert{$\sigma$}
		\item<only@2> nonconvex \alert{cone} $\cc \subset \cw^{2,\infty}$ defined by constants $n_{\init}$ and $\fC_0$
	\end{itemize}
	\vspace{-3ex}
	\only<1>{one may \alert{nonadaptively}}\only<2>{we \alert{adaptively}} construct a linear spline, $S(f,x_{0:n})$ with \only<1>{$x_i = a + i(b-a)/n,\ i=0:n$}\only<2>{$a = x_0 <x_1 < \cdots < x_n = b$}, satisfying $\forall f \in \only<1>{\cb}\only<2>{\cc}$ \only<2>{\cite{ChoEtal17a}}
	\begin{gather*}
	\norm{f - S(f,x_{0:n})} \overset{\text{\alert{always}}}{\le}  \max_{i=1:n}\frac{1}{8}(x_{i}-x_{i-1})^2\norm[{[x_{i-1},x_i]}]{f''} \only<1>{ \le \frac{(b-a)^2\alert{\sigma}}{8 n^2}} \overset{\text{\alert{goal}}}{\le} \varepsilon  \\
	\only<1>{\sqrt{\frac{\alert{\sigma}}{\varepsilon}} \asymp \comp(\varepsilon,\alert{\sigma},\cb) \le \cost(S,\varepsilon, \alert{\sigma}, \cb) = n+1 \\[-1ex] \hspace{30ex} = \left \lceil (b-a)\sqrt{\frac{\alert{\sigma}}{8\varepsilon}} \, \right \rceil + 1}
	\only<2>{\sqrt{\frac{\norm[\alert{\frac12}]{f''}}{\varepsilon}} \asymp \comp(\varepsilon,f, \cc) \le \cost(S, \varepsilon, f, \cc) = n+1 \asymp \sqrt{\frac{\norm[\alert{\frac12}]{f''}}{\varepsilon}} \\
		\comp\Bigl(\varepsilon,\sigma, \Bigl\{f \in \cw^{2,\infty}: \norm[\alert{\frac12}]{f''} \le \sigma\Bigr\}\Bigr) = \infty }
	\end{gather*}
	
	
\end{frame}

\begin{frame}
	\frametitle{State of the Art}
	
	\vspace{-3ex}
	
	\begin{itemize}
		\item Algorithms with \alert{theoretical justification} require bounds on the (semi-)norms of functions---impractical
		
		\item Popular adaptive algorithms have no sufficient conditions for success, e.g., \alert{Chebfun} \cite{TrefEtal16b} uses Chebyshev polynomials to approximate, integrate, optimize, etc.\ functions 
		
		\item Successful optimization algorithms require the input function to be \alert{convex}
		
		\item Guaranteed algorithms exist based on \alert{interval arithmetic} \cites{Rum99a, Rum10a}, but we do not want to follow that model
		
		\item \emph{We show that \alert{adaption does not help} for linear problems for the worst case setting [for a balanced, convex set of input functions] \ldots } \alert{\cite{Woz88a}}
		
		\item \ocite{PlaEtal08a} have approximation algorithms for functions with \alert{singularities}
		
		
	\end{itemize}
\end{frame}

\againframe<2>{Goal}

\begin{frame}
	\frametitle{Bounding $\norm[{[\alpha,\gamma]}]{f''}$}
	\northeaststuff{3.7}{2.2}{\includegraphics[width=7cm]{ProgramsImages/LinearSplineSecDeriv.eps}}
	
	\begin{multline*}
	\inf_{\alpha \le \xi < \zeta \le \gamma} \abs{\frac{f'(\zeta) - f'(\xi)}{\zeta - \xi}} = \norm[{-\infty, [\alpha,\gamma]}]{f''} \\
	\le 2\abs{f[\alpha,\beta,\gamma]} = 2 \abs{\frac{\frac{f(\gamma) - f(\beta)}{\gamma - \beta} - \frac{f(\beta) - f(\alpha)}{\beta - \alpha}}{\gamma - \alpha}} \alert{\leftarrow \text{ data}}\\
	\le \norm[{[\alpha,\gamma]}]{f''} = \sup_{\alpha \le \xi < \zeta \le \gamma} \abs{\frac{f'(\zeta) - f'(\xi)}{\zeta - \xi}}
	\end{multline*}
	\vspace{-4.5ex}
	
	\uncover<2>{We construct our \alert{adaptive} spline algorithm for functions in 
		
		\vspace{-6.2ex}
		\begin{multline*}
		\cc \ \text{``}\!\!=\!\!\text{''} \ \Bigl\{f : \norm[{[\alpha,\gamma]}]{f''} \le \max \bigl (\fC(h_{-}) \norm[{-\infty,[\gamma - h_-, \alpha]}]{f''}, \\[-1ex] \fC(h_{+})\norm[{-\infty,[\gamma,\alpha + h_+ ]}]{f''} \bigr), \ 
		\gamma - \alpha \le h_{\pm} < \fh\Bigr\}
		\end{multline*}}
\end{frame}

\begin{frame}
	\frametitle{Cone of Reasonable Functions, $\cc$}
	\northeaststuff{3.7}{3}{\includegraphics[width=7cm]{ProgramsImages/LinearSplineSecDeriv.eps}}
	
	\vspace{-10ex}
	
	\parbox{4.2cm}{
		\begin{align*}
		\norm[{-\infty, [\alpha,\gamma]}]{f''}
		& \le 2\abs{f[\alpha,\beta,\gamma]} \\ & \le \norm[{[\alpha,\gamma]}]{f''}
		\end{align*}
		
		\vspace{-1.5ex}
		Given $n_{\init} \ge 4$, $\fC_0 \ge 1$: 
		
		\vspace{-3.5ex}
		
		\begin{gather*}
		\fh  = \left \lceil \frac{3(b-a)}{n_{\init}-1} \right \rceil \\ \fC(h) = \frac{\fC_0 \fh}{\fh - h}
		\end{gather*}}
	
	\vspace{-4ex}
	
	We construct our \alert{adaptive} spline algorithm for functions in 
	
	\vspace{-5.5ex}	
	\begin{gather*}
	\cc =  \Bigl\{f : \norm[{[\alpha,\gamma]}]{f''} \le \max \bigl (B_{\pm}(f'',\alpha,\gamma,h_{\pm})  \bigr), \ \gamma - \alpha \le h_{\pm} < \fh\Bigr\} \\[-1ex]
	B_{-}(f'',\alpha,\gamma,h_-) = \begin{cases} \fC(h_{-}) \norm[{-\infty,[\gamma - h_-, \alpha]}]{f''}, & a \le \gamma - h_- \\
	0, & \text{otherwise} \end{cases} \\
	B_{+}(f'',\alpha,\gamma,h_+) = \begin{cases} \fC(h_{+})\norm[{-\infty,[\gamma,\alpha + h_+ ]}]{f''}, & \alpha + h_+ \le b \\
	0, & \text{otherwise} \end{cases} \\
	\end{gather*}
\end{frame}

\section{Approximation Algorithm}
\begin{frame}
	\frametitle{Local Spline Error Bounds for $\cc$}
	\northeaststuff{3.7}{3}{\includegraphics[width=7cm]{ProgramsImages/LinearSplineSecDeriv.eps}}
	
	\vspace{-8ex}
	
	\parbox{4.2cm}{
		Use divided differences to construct spline error bounds
		\begin{multline*}
		\norm[{[x_{i-1},x_i]}]{f - S(f,x_{0:n})} \\ \le \max_{\pm} \oerr_{i,\pm}(f)
		\end{multline*}
	}
	
	
	\vspace{-4ex}
	
	where
	
	\vspace{-4ex}	
	\begin{gather*}
	\oerr_{i,-}(f) = \begin{cases} \frac 14 (x_i-x_{i-1})^2\fC(x_i - x_{i-3}) \abs{f[x_{i-3},x_{i-2},x_{i-1}]}, & i \ge 3\\
	0, & \text{otherwise} \end{cases} \\
	\oerr_{i,+}(f) = \begin{cases} \frac 14 (x_i-x_{i-1})^2\fC(x_{i+2} - x_{i-1}) \abs{f[x_{i},x_{i+1},x_{i+2}]}, & i \le n-2 \\
	0, & \text{otherwise} \end{cases} \\
	\end{gather*}
\end{frame}

\begin{frame}
	\frametitle{Approximation Algorithm}
	\northeaststuff{3.7}{3}{\includegraphics[width=7cm]{ProgramsImages/sampling-funappxg.eps}}
	
	\vspace{-8ex}
	
	\parbox{4.2cm}{
		Given $n_{\init} \ge 4$, $\fC_0 \ge 1$: 
		
		\vspace{-3.5ex}
		
		\begin{gather*}
		\fh  = \left \lceil \frac{3(b-a)}{n_{\init}-1} \right \rceil \\ 
		\fC(h) = \frac{\fC_0 \fh}{\fh - h} \\
		n = n_{\init} \\
		x_i = a + i(b-a)/n
		\end{gather*}
	}
	
	
	\vspace{-4ex}
	
	\begin{description}
		\item[Step 1.] Compute data based $\oerr_{i\pm}(f)$ for $i = 1, \ldots, n$.
		\item[Step 2.] Construct $\ci$, the index set of subintervals that might be split:
		\begin{equation*}
		\ci = \bigl\{i \in 1\! :\! n \ : \ \oerr_{i\pm j,\mp}(f) > \varepsilon, \ j = 0, 1, 2\}
		\end{equation*}
		\item[Step 3.]  If $\ci = \emptyset$, return $S(f,x_{0:n})$ as the approximation satisfying the error tolerance.  Otherwise split those intervals in $\ci$ with largest width and go to Step 1. 
	\end{description}
	
\end{frame}

\begin{frame}
	\frametitle{Computational Cost of Function Approximation}
	\vspace{-6.5ex}
	\begin{align*}
	h_{\init}  &= \frac{b-a}{n_{\init}},  \quad  h_{\init} 2^{-\ell(x)} = \text{final width of subinterval containing } x \\
	I_{x}& =\text{interval containing $x$ with width } 10 \cdot h_{\init} 2^{-\ell(x)} \\
	\end{align*}
	\vspace{-6ex}
	\begin{align*}
	\cost(S,f,\varepsilon,\cc) 
	&= \int_a^b \frac{\dif x}{\text{final width of subinterval containing } \alert{x}} \,  + 1\\
	&= \int_a^b \frac{\dif x}{h_{\init} 2^{-\ell(\alert{x})}} \,  + 1\\
	&\le \int_a^b \sqrt{\frac{\fC\left(6\cdot 2^{-\ell(\alert{x})} h_{\init}\right) \norm[I_{\alert{x}}]{f''} }{2 \abstol}}  \, \dif x + 1\\
	& \sim \sqrt{\frac{\fC_0 \norm[\alert{\frac 12}]{f''}}{2\abstol}}  \quad \text{ as } \varepsilon \to 0 \\
	\comp(\varepsilon,f,\cc) &\ge \sqrt{\frac{(\fC_0-1)  \norm[\frac12]{f''} }{16(\fC_0+1)\varepsilon}} -1 \quad \text{via bump functions}
	\end{align*}
\end{frame}



\begin{frame}{Context}

\vspace{-5ex}

\begin{itemize}
	\item Linear \alert{solution} operator $\sol : \cf \to \cg$, input space $\cf$ contains functions defined on $\Omega \subseteq \reals^d$ \\
	\qquad e.g., $\sol(f) = f$, \quad $- \nabla^2 \sol(f) = f, \  \sol(f) = 0$  on boundary
	\item \alert{Approximation} $\app(f,n) = \sum_{i=1}^n L_i(f) g_i$, can sample linear functionals \\
	\qquad $\norm[\cg]{\sol(f) - \app(f,n)} \le \norm[\cf \to \cg]{\sol - \app(\cdot,n)} \norm[\cf]{f}$
	\item \alert{Algorithm} $\alg(f,\varepsilon) = \app\bigl(f,n^*(f,\varepsilon)\bigr)$ satisfying  
	$\norm[\cg]{\sol(f) - \alg(f,\varepsilon)} \le \varepsilon$ for all $f \in \alert{\cc} \subset \cf$

\uncover<2->{\item \alert{Solvability:} what $\cc$?  how  to determine $n^*(f,\varepsilon) \in \naturals$?  
	\only<3->{\smallscoop\only<3->{\hspace{-6.5ex}\raisebox{-1.5ex}{\color{red}\fontsize{40}{48}\selectfont $\times$}} or \smallcone}
\only<2>{
	
	\begin{itemize}
	\item $n^*(f,\varepsilon) = \min\{n : \norm[\cf \to \cg]{\sol - \app(\cdot,n)} \le \varepsilon / R\}$  if $f \in \smallscoop$ of radius $R$
	
	\item Alternatively, assume $f \in \smallcone$ and bound $ \norm[\cf]{f}$ or equivalent\footfullcite{HicEtal17a,KunEtal19a,DinHic20a} \\
	\emph{What you do not see is not much worse than what you see?}
\end{itemize}}

\only<3->{\item \alert{Optimality:} is $n^*(f,\varepsilon)$ essentially as small as possible?
	
	\item \alert{Tractability:} does $n^*(f,\varepsilon)$ depend nicely on $d$?
}
}

\end{itemize}

\end{frame}

\end{document}


\section{Algorithm}

\begin{frame}<1>[label = Setup]{
		\only<1>{General Linear Problems Defined on Series Spaces}%
		\only<2-4>{Adaptive Algorithm for General Linear Problems on Cone of Inputs}%
		\only<5>{Optimality of Algorithm}%
		\only<6>{Tractability  of Solving General Linear Problems on Cone of Inputs}%
		\footfullcite{DinEtal20a}}
	
	\vspace{-7ex}
	
	\begin{align*}
	\only<1-2>{
        \cf &:= \left \{ f = \sum_{i=1}^\infty \hf(\vk_i) u_{\vk_i} : \norm[\cf]{f} := \norm[\rho]{\left(\frac{\bigabs{\hf(\vk_i)}}{\lambda_{\vk_i}} \right)_{i =1}^\infty} \right \} \qquad 
		\begin{minipage}{4.2cm}\raggedright  
		    $\lambda_{\vk_1} \ge \lambda_{\vk_2} \ge \cdots > 0$ \\
		    \alert{$\vlambda$ affects convergence rate \&  \\ tractability}
		\end{minipage}\\
		\cg &: = \biggl \{ g = \sum_{i=1}^\infty \hg(\vk_i) v_{\vk_i} : \norm[\cg]{g} := \bignorm[\tau]{\hg}\biggr \}, \qquad
		\sol(f) = \sum_{i=1}^\infty \hf(\vk_i) v_{\vk_i}, \quad \tau \le \rho \\
		}
	    \uncover<2->{\cc_{\only<6>{\alert{d},}\only<1-5>{\vlambda}\only<6>{\alert{\vlambda_d}}, n_1, A} &: = \Biggl\{ f \in \cf : \norm[\cf]{f} \le A\norm[\rho]{\biggl ( \frac{\hf(\vk_i) }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}} \Biggr\} \qquad 
		\begin{minipage}{5.5cm}\raggedright 
		    \alert{pilot sample \\ bounds the norm of the input}
		\end{minipage} }
        \only<2-4>{\uncover<2-4>{\\
	        \app(f,n) &= \sum_{i=1}^{n} \hf(\vk_i) v_{\vk_i}\quad  \alert{\text{optimal for fixed }n}
	        \qquad \norm[\cg]{\sol(f) - \app(f,n)} \le  \ERRN 
	        }}
		\end{align*}
		\vspace{-2ex}
\only<2-4>{\uncover<2-4>{\begin{equation*}
    \ERRN := \underbrace{\left[ A^\rho \norm[\rho]{\left( \frac{\hf(\vk_i)}{\lambda_{\vk_i}} \right)_{i=1}^{n_1}}^\rho -  \norm[\rho]{\left(\frac{\hf(\vk_i)}{\lambda_{\vk_i}}\right)_{i=1}^n}^\rho \right]^{1/\rho}}_{\text{upper bound on } \norm[\cf]{f - \sum_{i=1}^{n} \hf(\vk_i) u_{\vk_i}}}
    \, 
    \underbrace{\bignorm[\rho']{\bigl( \lambda_{\vk_i}  \bigr)_{i = n+1}^{\infty}}}_{\norm[\cf \to \cg]{\sol - \app(\cdot,n)}}  
    \quad 
\begin{minipage}{2cm}
    $\displaystyle \frac 1\rho + \frac 1 {\rho'} = \frac 1 \tau $\\[1ex]
    \alert{\text{data-driven}}
\end{minipage}
\end{equation*}
}}
    \only<3-5>{\vspace{-2ex}
    \begin{align*}
	\alert<3>{\alg(f,\varepsilon)} & 
	\alert<3>{= \app(f,n^*(f,\varepsilon))} 
	\text{ for } \alert<3>{n^*(f,\varepsilon) = \min \{ n \in \naturals : \ERRN \le \varepsilon\} }
	\\ 
    \uncover<4->{
    \alert<4>{\COST(\alg,\cc_{\vlambda, n_1,A},\varepsilon,R)} 
    & \alert<4>{= \max \bigl \{ n^*(f,\varepsilon) : f \in \cc_{\vlambda, n_1, A} \cap \cb_R = \smallcone \cap \smallscoop \bigr\}} \\
    & \alert<4>{= \min \left \{n \ge n_1 : \bignorm[\rho']{\bigl(  \lambda_{\vk_i}  \bigr)_{i = n+1}^{\infty}} \,
    \le \varepsilon/[(A^\rho -1)^{1/\rho}R] \right \}}
		}
	\only<5->{ \\
	\alert{\COMP(\ca(\cc_{\vlambda, n_1,A},\varepsilon,R))} & = \min \{\COST(\alg',\cc_{\vlambda, n_1,A},\varepsilon,R)  : \alg' \in   \ca(\cc_{\vlambda, n_1,A})    \} \\
	& \ge \alert{\COST(\alg,\cc_{\vlambda, n_1,A},\omega \varepsilon,R)}, \qquad 
	\omega = \frac{2A(A^\rho -1)^{1/\rho}}{A - 1} > 1\\
	& \qquad \qquad \text{\alert{via fooling functions}}
	}
    \end{align*}
    \only<5>{$\alg$ is \alert{essentially optimal}}
    }
    \only<6>{For $\tau = \rho$ ($\rho'=\infty$), the problem $\sol_d :\cc_{\alert{d},\alert{\vlambda_d}, n_1, A} \to \cg_d$ is 
    \begin{description}
    \item[Strongly polynomial tractable] iff there exist $i_0 \in \naturals$ and $\eta > 0$ such that \quad
    $\displaystyle  \sup_{d\in\naturals} \sum_{i=i_0}^\infty \lambda_{d,\vk_i}^\eta < \infty$
    
    \item[Polynomial tractable] iff there exist $\eta_1, \eta_2 \ge 0$ and $\eta_3, K>0$ such that \qquad
 $
    \sup_{d\in\naturals} d^{-\eta_1}\, \sum_{i=\lceil K d^{\eta_2} \rceil}^\infty \lambda_{d,\vk_i}^{\eta_3} < \infty
 $
 \item[Weakly tractable] iff \quad
 $
  \sup_{d\in\naturals} \, \exp(-cd) \sum_{i=1}^\infty \exp\left(-c\left(\frac{1}{\lambda_{d,\vk_i}}\right)\right) <\infty$ \; for all $c>0$
 
    \end{description}
    Analogous results exist for $ \tau < \rho$ ($\rho' < \infty$)
    }

	
\end{frame}


\begin{frame}{Legendre and Chebyshev Bases for Function Approximation}
\vspace{-3ex}
	\begin{tabular}{>{\centering}m{0.18\textwidth}>{\centering}m{0.18\textwidth}>{\centering}m{0.18\textwidth}>{\centering}m{0.18\textwidth}>{\centering}m{0.18\textwidth}}
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_0.png}  &
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_1.png}  &
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_2.png}  &
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_3.png}  &
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_4.png} 
	\tabularnewline[-7ex]
	Legendre
	\tabularnewline
	\tabularnewline
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_1_1.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_1_2.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_1_3.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_2_2.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Legendre_Degree_2_3.png} 
\tabularnewline[0ex]
		\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_0.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_1.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_2.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_3.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_4.png} 
\tabularnewline[-7ex]
Chebyshev \tabularnewline
\tabularnewline
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_1_1.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_1_2.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_1_3.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_2_2.png}  &
\includegraphics[width =0.18\textwidth]{ProgramsImages/Chebyshev_Degree_2_3.png} 
	\end{tabular}
\end{frame}

\againframe<2-4>{Setup}

\section{Optimality}

\againframe<5>{Setup}

\section{Tractability}

\againframe<6->{Setup}

\section{Alternate Cone}

\begin{frame}
	\frametitle{An Alternate Cone, Similar to Our Cone}
	\vspace{-5.5ex}
	\begin{gather*}
	\norm[\cf]{f} : = \norm[\rho]{\biggl ( \frac{\hf(\vk_i) }{\lambda_{\vk_i}} \biggr) _{i=1}^\infty}, \quad \lambda_{\vk_i} \downarrow 0, \qquad
	\norm[\cf']{f} : = \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i}}{\lambda_{\vk_i}} \biggr) _{i=1}^\infty},  
	\quad \zeta_{\vk_1} = 1, \; \zeta_{\vk_i} \downarrow
\\
	\underbrace{\cc_{\vlambda, n_1, A} : = \Biggl\{ f \in \cf : \norm[\cf]{f} \le A\norm[\rho]{\biggl ( \frac{\hf(\vk_i) }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}} \Biggr\} }_{\text{pilot sample bounds the norm of the input}}
	\qquad 
	\alert{\underbrace{\cc'_{\vlambda, \vzeta, A'} : = \bigl \{ f \in \cf : \norm[\cf]{f} \le A'\norm[\cf']{f} \bigr \} }_{\text{stronger norm is bounded above by weaker norm}}}
	\end{gather*}

		\begin{gather*}
\alert{\cc'_{\vlambda, \vzeta, A'} \subseteq \cc_{\vlambda, n_1, A}}  \qquad 
\text{for } A \ge A'\left[\frac{1 + \zeta_{\vk_{n_1+1}}^\rho}{1 - (\zeta_{\vk_{n_1+1}}A')^\rho}\right]^{1/\rho},  \quad \zeta_{\vk_{n_1+1}}A' < 1  \\
\alert{\cc_{\vlambda, n_1, A} \subseteq \cc'_{\vlambda, \vzeta, A''} }  \qquad 
\text{for } A \le A'' \zeta_{\vk_{n_1}}
		\end{gather*}

\end{frame}



\begin{frame}
	\frametitle{An Alternative Cone, Similar to Our Cone}
	\vspace{-2ex}
	For any $f \in 	\cc'_{\vlambda, \vzeta, A'} $ it follows that 
	\begin{align*}
	\norm[\cf']{f}^\rho- \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i} }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}^\rho 
	& = \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i} }{\lambda_{\vk_i}} \biggr) _{i=n_1 + 1}^{\infty}}^\rho \\
	& \le \zeta_{\vk_{n_1 +1}}^\rho \norm[\rho]{\biggl ( \frac{\hf(\vk_i) }{\lambda_{\vk_i}} \biggr) _{i=n_1 + 1}^{\infty}}^\rho 
	= \zeta_{\vk_{n_1 +1}}^\rho \left[ \norm[\cf]{f}^\rho - \norm[\rho]{\biggl ( \frac{\hf(\vk_i) }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}^\rho \right ]\\	
	& \le \zeta_{\vk_{n_1 +1}}^\rho \left[ A'{}^\rho \norm[\cf']{f}^\rho -  \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i}}{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}^\rho \right ]
	\uncover<2>{\\
		\norm[\cf']{f}^\rho & \le \frac{1 + \zeta_{\vk_{n_1 +1}}^\rho}{1 - (\zeta_{\vk_{n_1 +1}}A')^\rho} \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i} }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}^\rho \\
		\norm[\cf]{f} & \le A' \norm[\cf']{f} 
		\le A'\left[\frac{1 + \zeta_{\vk_{n_1 +1}}^\rho}{1 - (\zeta_{\vk_{n_1 +1}}A')^\rho}\right]^{1/\rho} \norm[\rho]{\biggl ( \frac{\hf(\vk_i)}{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}
	}
	\end{align*}
\end{frame}


\begin{frame}
	\frametitle{An Alternate Cone, Similar to Our Cone}
	\vspace{-2ex}
	For any $f \in \cc_{\vlambda, n_1, A} $ it follows that 
	\begin{equation*}
\norm[\cf]{f} \le A \norm[\rho]{\biggl ( \frac{\hf(\vk_i)}{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}} 
    \le  \frac{A}{\zeta_{\vk_{n_1}}} \norm[\rho]{\biggl ( \frac{\hf(\vk_i) \zeta_{\vk_i} }{\lambda_{\vk_i}} \biggr) _{i=1}^{n_1}}
    \le \frac{A}{\zeta_{\vk_{n_1}}} \norm[\cf']{f}
	\end{equation*}
\end{frame}

\section{Summary}

\begin{frame}
	\frametitle{Conclusion}
	
	\vspace{-4ex}
	
	\alert{Summary}
	
	\vspace{-3ex}
	\begin{itemize}
		\item Adaptive algorithms can be constructed for non-convex, symmetric cones of inputs
		
		\item One possible cone assumes that a pilot sample tells you enough about the norm of the input
		
		\item Information cost and complexity depend on the norm of the input, but the adaptive algorithm does not  know this norm
		
		\item There are tractability results for problems defined on cones
		
		\item Our algorithm can be extended to infer the $\lambda_{\vk}$ in terms of coordinate weights\footfullcite{DinEtal20a}
	\end{itemize}

\alert{Further Work}

	\vspace{-3ex}
	\begin{itemize}
	\item Need to develop and analyze algorithms based on function values, not series coefficients
	
\end{itemize}


\end{frame}

\begin{frame}{Cheng and Sandu Function\footfullcite{VirLib17a}}
	\vspace{-9ex}
	\begin{gather*}
	\text{Function values for data \qquad Chebyshev polynomial basis}, \qquad \lambda_{\vk} \text{ \alert{inferred} }, 
	\end{gather*}
	
	
	\centerline{\includegraphics[height = 5cm]{ProgramsImages/sim_eval_results_chsan10_d6_sflg0ErrN.eps}}
	
\end{frame}



\finalthanksnote{These slides are  available at \\  \href{https://speakerdeck.com/fjhickernell/mcm-2019-pilot-sample}{\nolinkurl{speakerdeck.com/fjhickernell/mcm-2019-pilot-sample}}}


\thankyouframe

\begin{frame}[allowframebreaks]
	\frametitle{References}
\printbibliography
\end{frame}




\end{document}



